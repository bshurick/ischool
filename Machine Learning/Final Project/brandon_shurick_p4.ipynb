{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Project \n",
    "======\n",
    "\n",
    "Kaggle Competition \n",
    "-----\n",
    "\n",
    "For this project I chose to do the active competition [San Francisco Crime Classification](https://www.kaggle.com/c/sf-crime/). I'll test out a number of different algorithms with test data. I am interested in this type of analysis as it is data science that contributes to the common good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Python functions\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime\n",
    "import re, math\n",
    "\n",
    "# Patsy \n",
    "from patsy import dmatrices\n",
    "\n",
    "# sklearn functions\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.preprocessing import OneHotEncoder, Imputer, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, scale\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report\\\n",
    "                        , f1_score, accuracy_score \n",
    "from sklearn.feature_selection import SelectKBest\\\n",
    "                        , SelectPercentile, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.decomposition import PCA\\\n",
    "                                , TruncatedSVD  #for sparse matrices\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.grid_search import GridSearchCV \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Make plots BIG\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "# GIS functionality\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import fiona\n",
    "import pysal \n",
    "from pyproj import Proj\n",
    "from pysal.cg.shapes import Point\n",
    "from pysal.cg.locators import PolygonLocator, PointLocator, BruteForcePointLocator \n",
    "from pysal.cg.sphere import arcdist\n",
    "\n",
    "# Multiprocessing \n",
    "import multiprocessing\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "FORMAT = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(format=FORMAT)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiColumnLabelEncoder:\n",
    "    ''' Create a class that encodes\n",
    "        labels for a matrix of data\n",
    "    '''\n",
    "    def __init__(self, columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        out = dict()\n",
    "        if self.columns: out['columns'] = columns\n",
    "        return out\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). \n",
    "        '''\n",
    "        numerics = [np.float16, np.float32, np.float64]\n",
    "        ints = [np.int16, np.int32, np.int64]\n",
    "        output = X.copy()\n",
    "        '''\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                if col.dtype not in numerics+ints:\n",
    "                    output[col] = LabelEncoder().fit_transform(output[col])\n",
    "                elif col.dtype not in ints:\n",
    "                    output[col] = scale(output[col])\n",
    "        else:\n",
    "        '''\n",
    "        try:\n",
    "            for colname,col in output.iteritems():\n",
    "                if col.dtype not in numerics+ints:\n",
    "                    # Turn text columns into ints\n",
    "                    output[colname] = LabelEncoder().fit_transform(output[colname])\n",
    "                elif col.dtype in numerics:\n",
    "                    # handle floats with scaling\n",
    "                    # output[colname] = scale(output[colname])\n",
    "                    pass \n",
    "                elif col.dtype in ints:\n",
    "                    pass # leave integers alone\n",
    "        except:\n",
    "            output = LabelEncoder().fit_transform(output)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049, 9)\n"
     ]
    }
   ],
   "source": [
    "train_raw = pd.read_csv('Data/train.csv')\n",
    "test_raw = pd.read_csv('Data/test.csv')\n",
    "sample_submission = pd.read_csv('Data/sampleSubmission.csv')\n",
    "print train_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Submit first (bad) try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Dates</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>PdDistrict</th>\n",
       "      <th>Address</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-05-10 23:59:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>2000 Block of THOMAS AV</td>\n",
       "      <td>-122.399588</td>\n",
       "      <td>37.735051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-05-10 23:51:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>BAYVIEW</td>\n",
       "      <td>3RD ST / REVERE AV</td>\n",
       "      <td>-122.391523</td>\n",
       "      <td>37.732432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-05-10 23:50:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>2000 Block of GOUGH ST</td>\n",
       "      <td>-122.426002</td>\n",
       "      <td>37.792212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-05-10 23:45:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>INGLESIDE</td>\n",
       "      <td>4700 Block of MISSION ST</td>\n",
       "      <td>-122.437394</td>\n",
       "      <td>37.721412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-05-10 23:45:00</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>INGLESIDE</td>\n",
       "      <td>4700 Block of MISSION ST</td>\n",
       "      <td>-122.437394</td>\n",
       "      <td>37.721412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                Dates DayOfWeek PdDistrict                   Address  \\\n",
       "0   0  2015-05-10 23:59:00    Sunday    BAYVIEW   2000 Block of THOMAS AV   \n",
       "1   1  2015-05-10 23:51:00    Sunday    BAYVIEW        3RD ST / REVERE AV   \n",
       "2   2  2015-05-10 23:50:00    Sunday   NORTHERN    2000 Block of GOUGH ST   \n",
       "3   3  2015-05-10 23:45:00    Sunday  INGLESIDE  4700 Block of MISSION ST   \n",
       "4   4  2015-05-10 23:45:00    Sunday  INGLESIDE  4700 Block of MISSION ST   \n",
       "\n",
       "            X          Y  \n",
       "0 -122.399588  37.735051  \n",
       "1 -122.391523  37.732432  \n",
       "2 -122.426002  37.792212  \n",
       "3 -122.437394  37.721412  \n",
       "4 -122.437394  37.721412  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ARSON</th>\n",
       "      <th>ASSAULT</th>\n",
       "      <th>BAD CHECKS</th>\n",
       "      <th>BRIBERY</th>\n",
       "      <th>BURGLARY</th>\n",
       "      <th>DISORDERLY CONDUCT</th>\n",
       "      <th>DRIVING UNDER THE INFLUENCE</th>\n",
       "      <th>DRUG/NARCOTIC</th>\n",
       "      <th>DRUNKENNESS</th>\n",
       "      <th>...</th>\n",
       "      <th>SEX OFFENSES NON FORCIBLE</th>\n",
       "      <th>STOLEN PROPERTY</th>\n",
       "      <th>SUICIDE</th>\n",
       "      <th>SUSPICIOUS OCC</th>\n",
       "      <th>TREA</th>\n",
       "      <th>TRESPASS</th>\n",
       "      <th>VANDALISM</th>\n",
       "      <th>VEHICLE THEFT</th>\n",
       "      <th>WARRANTS</th>\n",
       "      <th>WEAPON LAWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  ARSON  ASSAULT  BAD CHECKS  BRIBERY  BURGLARY  DISORDERLY CONDUCT  \\\n",
       "0   0      0        0           0        0         0                   0   \n",
       "1   1      0        0           0        0         0                   0   \n",
       "2   2      0        0           0        0         0                   0   \n",
       "3   3      0        0           0        0         0                   0   \n",
       "4   4      0        0           0        0         0                   0   \n",
       "\n",
       "   DRIVING UNDER THE INFLUENCE  DRUG/NARCOTIC  DRUNKENNESS     ...       \\\n",
       "0                            0              0            0     ...        \n",
       "1                            0              0            0     ...        \n",
       "2                            0              0            0     ...        \n",
       "3                            0              0            0     ...        \n",
       "4                            0              0            0     ...        \n",
       "\n",
       "   SEX OFFENSES NON FORCIBLE  STOLEN PROPERTY  SUICIDE  SUSPICIOUS OCC  TREA  \\\n",
       "0                          0                0        0               0     0   \n",
       "1                          0                0        0               0     0   \n",
       "2                          0                0        0               0     0   \n",
       "3                          0                0        0               0     0   \n",
       "4                          0                0        0               0     0   \n",
       "\n",
       "   TRESPASS  VANDALISM  VEHICLE THEFT  WARRANTS  WEAPON LAWS  \n",
       "0         0          0              0         1            0  \n",
       "1         0          0              0         1            0  \n",
       "2         0          0              0         1            0  \n",
       "3         0          0              0         1            0  \n",
       "4         0          0              0         1            0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data\\\n",
    ",train_labels = train_raw[['DayOfWeek'\\\n",
    "                                     ,'PdDistrict'\\\n",
    "                                     ,'Address'\\\n",
    "                                     ,'X'\\\n",
    "                                     ,'Y']][2001:]\\\n",
    "                            ,train_raw['Category'][2001:]\n",
    "dev_data\\\n",
    ",dev_labels = train_raw[['DayOfWeek'\\\n",
    "                                 ,'PdDistrict'\\\n",
    "                                 ,'Address'\\\n",
    "                                 ,'X'\\\n",
    "                                 ,'Y']][:1000]\\\n",
    "                            ,train_raw['Category'][:1000]\n",
    "dev_test_data\\\n",
    ",dev_test_labels = train_raw[['DayOfWeek'\\\n",
    "                                 ,'PdDistrict'\\\n",
    "                                 ,'Address'\\\n",
    "                                 ,'X'\\\n",
    "                                 ,'Y']][1001:2000]\\\n",
    "                            ,train_raw['Category'][1001:2000]\n",
    "test_data = test_raw[['DayOfWeek'\\\n",
    "                     ,'PdDistrict'\\\n",
    "                     ,'Address'\\\n",
    "                     ,'X'\\\n",
    "                     ,'Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier() #leaving with default settings for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data needs to be preprocessed for RF in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def recode(df):\n",
    "    numerics = ['float16', 'float32', 'float64']\n",
    "    ints = ['int16', 'int32', 'int64']\n",
    "    for i in range(df.shape[1]):\n",
    "        t = df.iloc[:,i].dtype\n",
    "        if t not in numerics or ints:\n",
    "            le = LabelEncoder()\n",
    "            le.fit(df.iloc[:,i])\n",
    "            df.iloc[:,i] = le.transform(df.iloc[:,i])\n",
    "        elif t not in ints:\n",
    "            df.iloc[:,i] = scale(df.iloc[:,i])\n",
    "    return df\n",
    "\n",
    "def recode_labels(df):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df)\n",
    "    return le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_data = recode(dev_data)\n",
    "dev_label_le = recode_labels(np.concatenate((dev_labels,dev_test_labels),axis=1))\n",
    "dev_labels = dev_label_le.transform(dev_labels)\n",
    "dev_test_data = recode(dev_test_data)\n",
    "dev_test_labels = dev_label_le.transform(dev_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(dev_data,dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = rf.predict(dev_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASSAULT' 'LARCENY/THEFT' 'SUSPICIOUS OCC' 'VEHICLE THEFT' 'VEHICLE THEFT'] \n",
      "['OTHER OFFENSES' 'LARCENY/THEFT' 'LARCENY/THEFT' 'WARRANTS' 'WARRANTS']\n"
     ]
    }
   ],
   "source": [
    "actual_predictions = dev_label_le.inverse_transform(predictions)\n",
    "actual_labels = dev_label_le.inverse_transform(dev_test_labels)\n",
    "print actual_predictions[:5],'\\n',actual_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Accuracy: 21.3%\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.sum((actual_predictions == actual_labels)*1.0)/len(dev_labels)\n",
    "print 'Dev Accuracy: {0}%'.format(accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train with real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/pandas/core/indexing.py:415: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = recode(train_data), recode(test_data)\n",
    "train_le = recode_labels(train_labels)\n",
    "train_labels = train_le.transform(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9fd32e408bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 273\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_counts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    302\u001b[0m                                            max_leaf_nodes)\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make predictions and format for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = rf.predict(test_data)\n",
    "actual_predictions = train_le.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = len(test_data)\n",
    "submission = np.zeros((l,40),dtype=np.int32)\n",
    "submission[:,0] = range(l)\n",
    "cols = train_le.classes_ \n",
    "for i,c in enumerate(cols):\n",
    "    submission[:,i+1] = actual_predictions == c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_cols = ['Id']\n",
    "submission_cols.extend(cols)\n",
    "submission_df = pd.DataFrame(submission,columns=submission_cols)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save data to csv file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('Data/submission_file1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt: Rank 208, score 26.95890, tested accuracy of ~20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_test_data\\\n",
    ",dev_test_labels = train_raw[['DayOfWeek'\\\n",
    "                                 ,'PdDistrict'\\\n",
    "                                 ,'Address'\\\n",
    "                                 ,'X'\\\n",
    "                                 ,'Y']][1001:2000]\\\n",
    "                            ,train_raw['Category'][1001:2000]\n",
    "dev_test_data = recode(dev_test_data)\n",
    "predictions = rf.predict(dev_test_data)\n",
    "actual_predictions = train_le.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = confusion_matrix(dev_test_labels,actual_predictions)\n",
    "plt.imshow(conf, cmap='binary',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(dev_test_labels, actual_predictions, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since samples are not weighted well in terms of categories, the model seems to be having difficulties with categories that have a larger amount of observations - likely because all of the conditions used to predict crimes are seen more frequently for the samples with more observations. For example, there is no predictions that are Liqour Law because this is an infrequent category in the training data. To make the model generalize better it will be necessary to resample the dataset so that possible outcomes are represented more evenly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Tests to increase accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: see appendix for discussion about using the training dataset vs. using the publicly available crimes dataset. The latter essentially includes all of the crimes in both the training and testing set of data. YES this feels a lot like cheating (although they have no rules against using publicly-available data); however, there is a holdout set of about 70K records in the provided test dataset that they will probably use for evaluating the final models, and without using the full dataset of crimes you are at a severe disadvantage even on that 70K dataset compared to someone who utilizes all available data. I made the choice (like many others) to use this data instead of the provided training data to build my final model. My own accuracy is measured using a holdout set, so I am not overfitting in my own testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Load of data started')\n",
    "# train_raw = pd.read_csv('Data/train.csv')\n",
    "train_raw = pd.read_csv('/Users/bshur/School/Machine Learning/\\\n",
    "Final Project/Data/SFPD_Incidents_-_from_1_January_2003.csv')\n",
    "train_raw['Dates'] = pd.to_datetime(train_raw['Date']\\\n",
    "                                 +' '\\\n",
    "                                 +train_raw['Time']\\\n",
    "                                 ,format='%m/%d/%Y %H:%M')\n",
    "test_raw = pd.read_csv('Data/test.csv')\n",
    "logger.info('Load of data finished')\n",
    "print train_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep dive into crimes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print train_raw.groupby(['Category']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_descripts(cat,first_n=10):\n",
    "    ''' A function to evaluate descriptions for a category \n",
    "        sorted by the number of crimes\n",
    "    '''\n",
    "    g = train_raw[train_raw['Category']==cat]\\\n",
    "        [['Category','Descript']]\\\n",
    "        .groupby(['Category','Descript']).agg(len)\n",
    "    for x in sorted(zip(g.index,g.values),key=lambda x: x[1], reverse=True)[:first_n]:\n",
    "        print x\n",
    "\n",
    "show_descripts('SUSPICIOUS OCC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've noticed that categories aren't the best at describing the data in terms a model would understand. Some categories have a lot of crimes included that would make it difficult to have an accurate model, no matter what supplemental data sources are attached to the training set. Below I am looking to create a smaller number of 'meta-categories' that will be easier to predict by grouping individual categories and by including individual descriptions into appropriate meta-categories that will be easier for a model to interpret. \n",
    "\n",
    "After re-classifying the crimes for the training set, I will attempt to build a prediction model that will be used to predict meta-categories for the test set - which will then be utilized as an additional feature in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def collar_crimes(x,y):\n",
    "    ''' Add a meta category for \n",
    "        crimes based on the skills\n",
    "        required\n",
    "    '''\n",
    "    blue_collar_violent = [ \n",
    "                   \"ASSAULT\"\n",
    "                   , \"KIDNAPPING\"\n",
    "                   , \"ARSON\"\n",
    "                   , 'DOMESTIC VIOLENCE'\n",
    "                   , 'GANG ACTIVITY'\n",
    "                  ]\n",
    "    blue_collar_other = [\n",
    "                \"VANDALISM\"\n",
    "                ,\"DISORDERLY CONDUCT\"\n",
    "                ,\"TRESPASS\"\n",
    "                ,'TREA'\n",
    "               , 'LOITERING'\n",
    "                ,'RESISTING ARREST'\n",
    "                ,'PROBATION VIOLATION'\n",
    "                ,'PROBATION VIOLATION'\n",
    "                ,'VIOLATION OF RESTRAINING ORDER'\n",
    "                ,'PAROLE VIOLATION'\n",
    "    ]\n",
    "    sex_crimes = [\n",
    "            'SEX OFFENSES FORCIBLE',\n",
    "            'PORNOGRAPHY/OBSCENE MAT',\n",
    "            'SEX OFFENSES NON FORCIBLE',\n",
    "            'PROSTITUTION'\n",
    "        ]\n",
    "    alcohol = [\n",
    "        'DRIVING UNDER THE INFLUENCE',\n",
    "        'DRUNKENNESS',\n",
    "        'LIQUOR LAWS'\n",
    "    ]\n",
    "    drug = ['DRUG/NARCOTIC']\n",
    "    theft = [\n",
    "        'LARCENY/THEFT',\n",
    "         'STOLEN PROPERTY',\n",
    "         \"ROBBERY\",\n",
    "         'CREDIT CARD, THEFT BY USE OF',\n",
    "        'FRAUDULENT USE OF AUTOMATED TELLER CARD',\n",
    "        'BURGLARY'\n",
    "    ]\n",
    "    vehicle = [\n",
    "        'RECOVERED VEHICLE',\n",
    "        'VEHICLE THEFT',\n",
    "        'DRIVERS LICENSE, SUSPENDED OR REVOKED',\n",
    "        'TRAFFIC VIOLATION',\n",
    "        'TRAFFIC VIOLATION ARREST',\n",
    "        'DRIVERS LICENSE, SUSPENDED OR REVOKED',\n",
    "        'LOST/STOLEN LICENSE PLATE',\n",
    "        'IMPOUNDED VEHICLE',\n",
    "        'TRAFFIC ACCIDENT',\n",
    "        'MALICIOUS MISCHIEF, VANDALISM OF VEHICLES'\n",
    "    ]\n",
    "    noncrime = [\n",
    "        'MISSING PERSON',\n",
    "        'RUNAWAY',\n",
    "        'SUICIDE',\n",
    "        'NON-CRIMINAL',\n",
    "        'SUSPICIOUS OCC'\n",
    "    ]\n",
    "    white_collar = [ \n",
    "        \"FRAUD\"\n",
    "       , \"FORGERY/COUNTERFEITING\"\n",
    "       , \"BAD CHECKS\" \n",
    "       , \"EXTORTION\"\n",
    "       , \"EMBEZZLEMENT\"\n",
    "       , \"BRIBERY\"\n",
    "        , 'CONSPIRACY'\n",
    "    ]\n",
    "    if x in blue_collar_violent or y in blue_collar_violent: return 1\n",
    "    elif x in sex_crimes or y in sex_crimes: return 2\n",
    "    elif x in alcohol or y in alcohol: return 3\n",
    "    elif x in drug or y in drug: return 4\n",
    "    elif x in theft or y in theft: return 5\n",
    "    elif x in vehicle or y in vehicle: return 6\n",
    "    elif x in noncrime or y in noncrime: return 7\n",
    "    elif x in white_collar or y in white_collar: return 8\n",
    "    elif x in blue_collar_other or y in blue_collar_other: return 9\n",
    "    else: return 10\n",
    "collar_crimes = np.vectorize(collar_crimes,otypes=[np.int64])\n",
    "logger.info('Creation of collar_id started')\n",
    "train_raw['collar_id'] = collar_crimes(train_raw['Category'],train_raw['Descript'])\n",
    "logger.info('Creation of collar_id ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_newcategories(col,first_n=10):\n",
    "    ''' Evaluate how crimes are fit into the new \n",
    "        categories defined above\n",
    "    '''\n",
    "    g = train_raw[train_raw['collar_id']==col]\\\n",
    "        [['Category','Descript']]\\\n",
    "        .groupby(['Category','Descript']).agg(len)\n",
    "    for x in sorted(zip(g.index,g.values),key=lambda x: x[1], reverse=True)[:first_n]:\n",
    "        print x\n",
    "\n",
    "show_newcategories(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing I've noticed is how some categories have minimal amounts of crimes which makes it difficult to build a model because there is such a strong bias in predicting crimes that occur the most often. I am creating a sampling methodology that samples with replacement in order to gather a more even amount of observations in each dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gather counts of each category \n",
    "g = train_raw[['Category','Descript']].groupby(['Category']).agg(len)\n",
    "group_cnts = pd.DataFrame({'Category':np.array(g.index).T,'Value':np.array(g.T)[0]})\n",
    "print group_cnts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add time of day \n",
    "\n",
    "Since a timestamp is not good for the generalization of the model, attempt bucketing of hours within each day, and break off day of month and year of crime and separate dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ceil = np.vectorize(math.ceil)\n",
    "    \n",
    "logger.info('Datetime conversion started')\n",
    "train_raw['Dates'] = pd.to_datetime(train_raw['Dates'])\n",
    "test_raw['Dates'] = pd.to_datetime(test_raw['Dates'])\n",
    "\n",
    "logger.info('DaySegment creation started')\n",
    "train_raw['DaySegment'] = ceil((train_raw['Dates'].dt.hour+1)/4).astype(np.int)\n",
    "test_raw['DaySegment'] = ceil((test_raw['Dates'].dt.hour+1)/4).astype(np.int)\n",
    "\n",
    "logger.info('TimeOfDay creation started')\n",
    "train_raw['TimeOfDay'] = train_raw['Dates'].dt.hour\n",
    "test_raw['TimeOfDay'] = test_raw['Dates'].dt.hour\n",
    "\n",
    "logger.info('DayOfMonth creation started')\n",
    "train_raw['DayOfMonth'] = train_raw['Dates'].dt.day\n",
    "test_raw['DayOfMonth'] = test_raw['Dates'].dt.day\n",
    "\n",
    "logger.info('Year creation started')\n",
    "train_raw['Year'] = train_raw['Dates'].dt.year\n",
    "test_raw['Year'] = test_raw['Dates'].dt.year\n",
    "\n",
    "logger.info('Month creation started')\n",
    "train_raw['Month'] = train_raw['Dates'].dt.month\n",
    "test_raw['Month'] = test_raw['Dates'].dt.month\n",
    "\n",
    "logger.info('YearQtr creation started')\n",
    "train_raw['YearQtr'] = train_raw['Dates'].dt.year*100\\\n",
    "    +ceil(train_raw['Dates'].dt.month/4).astype(np.int)\n",
    "test_raw['YearQtr'] = test_raw['Dates'].dt.year*100\\\n",
    "    +ceil(test_raw['Dates'].dt.month/4).astype(np.int)\n",
    "\n",
    "logger.info('YearSegment creation started')\n",
    "train_raw['YearSegment'] = ceil(train_raw['Dates'].dt.month/4).astype(np.int)\n",
    "test_raw['YearSegment'] =  ceil(test_raw['Dates'].dt.month/4).astype(np.int)\n",
    "logger.info('Date feature processing ended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add clustering based on lat/lon and time of day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, by using the exact location of the crime, the model does not generalize very well. I KMeans to segment training data into clusters based on location, time of day, and year and add both the cluster label and distance from centroid as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_clusters(range_n_clusters,fields=['X','Y','YearSegment','Year']):\n",
    "    km_models = []\n",
    "    i_scores = []\n",
    "    le = MultiColumnLabelEncoder()\n",
    "    tr = train_raw[fields].copy()\n",
    "    tr = le.fit_transform(tr)\n",
    "    logger.info('Cluster test starting'.format(n_clusters))\n",
    "    for n_clusters in range(range_n_clusters):\n",
    "       \n",
    "        if n_clusters>1:\n",
    "            km = KMeans(n_clusters=n_clusters, random_state=5)\n",
    "            km.fit(tr)\n",
    "            km_models.append(km)\n",
    "            inertia = km.inertia_ \n",
    "            print 'For {0}, inertia = {1}'.format(\n",
    "                n_clusters, inertia\n",
    "            )\n",
    "            i_scores.append(inertia)\n",
    "       \n",
    "    # plot results\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('KMeans inertia values')\n",
    "    ax.set_xlabel('Clusters')\n",
    "    ax.set_ylabel('Inertia')\n",
    "    ax.plot([i for i in range(range_n_clusters) \\\n",
    "             if i>1],i_scores,'-', linewidth=2)\n",
    "    plt.show()\n",
    "    \n",
    "test_clusters(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the clustering process, I found that there are a few points with what must be default values of lat/lon coordinates: 90,-120.5. Those are values that aren't interpretable by GIS packages and cause significant issues with clustering as well, so I have to manually impute them with better default values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Manually impute bad X,Y values as \n",
    "train_raw.loc[train_raw['X']==-120.5,['X']] = np.mean(train_raw['X'])\n",
    "train_raw.loc[train_raw['Y']==90,['Y']] = np.mean(train_raw['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reload data in case of changes\n",
    "logger.info('Clustering started')\n",
    "le = MultiColumnLabelEncoder()\n",
    "nrm = StandardScaler()\n",
    "tr = train_raw[['X','Y','TimeOfDay','YearQtr']].copy()\n",
    "pl = Pipeline([('le',le),('nrm',nrm)])\n",
    "tr = pl.fit_transform(tr)\n",
    "\n",
    "# Set k\n",
    "k = 20\n",
    "\n",
    "# Initialize Kmeans model\n",
    "km = KMeans(n_clusters=k)\n",
    "logger.info('Clustering training data')\n",
    "train_raw['KMcluster'] = km.fit_predict(tr)\n",
    "\n",
    "# Calculate distances\n",
    "logger.info('Clustering distance calculation for training data')\n",
    "distances = km.transform(tr)\n",
    "train_raw['KMdistance'] = np.min(distances,axis=1)\n",
    "\n",
    "# Predict for test dataset\n",
    "logger.info('Clustering training data')\n",
    "tr = test_raw[['X','Y','TimeOfDay','YearQtr']].copy()\n",
    "tr = le.transform(tr)\n",
    "test_raw['KMcluster'] = km.predict(tr)\n",
    "logger.info('Clustering distance calculation for training data')\n",
    "distances = km.transform(tr)\n",
    "test_raw['KMdistance'] = np.min(distances,axis=1)\n",
    "logger.info('Clustering finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_clustercat(cluster):\n",
    "    ''' A function that shows top crimes in each cluster '''\n",
    "    g = train_raw[train_raw['KMcluster']==cluster].\\\n",
    "        groupby(['Category','KMcluster'])['Category'].agg(len)\n",
    "    for x in sorted(zip(g.index,g.values),key=lambda x: \\\n",
    "                    (x[0][1],x[1]), reverse=True)[:10]:\n",
    "        print x\n",
    "    \n",
    "show_clustercat(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_clusters():\n",
    "    ''' Show all clusters in individual scatterplots '''\n",
    "    f, axarr = plt.subplots(4, 5, sharex=True, sharey=True)\n",
    "    le = MultiColumnLabelEncoder()\n",
    "    nrm = StandardScaler()\n",
    "    tr = train_raw[['X','Y','TimeOfDay','YearQtr']].copy()\n",
    "    pl = Pipeline([('le',le),('nrm',nrm)])\n",
    "    tr = pl.fit_transform(tr)\n",
    "    pca = PCA(n_components=2)\n",
    "    X = pca.fit_transform(tr)\n",
    "    K = np.array(train_raw['KMcluster'])\n",
    "    for i in range(20):\n",
    "        if i<5: e=0\n",
    "        elif i<10: e=1\n",
    "        elif i<15: e=2\n",
    "        else: e=3\n",
    "        z = i - 5*e\n",
    "        axarr[e, z].plot(X[K==i,0]\\\n",
    "                            ,X[K==i,1]\\\n",
    "                            ,'bo')\n",
    "        axarr[e, z].set_title('K:{}'.format(i))\n",
    "        plt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)\n",
    "        plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)\n",
    "\n",
    "plot_clusters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_fields = [#'Category',\n",
    "                 'DayOfWeek',\n",
    "                 'PdDistrict',\n",
    "                 'DaySegment',\n",
    "                 'TimeOfDay',\n",
    "                 'DayOfMonth',\n",
    "                 'Year',\n",
    "                 'YearSegment',\n",
    "                 'X',\n",
    "                 'Y',\n",
    "                 'KMcluster',\n",
    "                 'KMdistance']\n",
    "categorical_fields = [#'Category',\n",
    "                 'DayOfWeek',\n",
    "                 'PdDistrict',\n",
    "                 'DaySegment',\n",
    "                 'TimeOfDay',\n",
    "                 'DayOfMonth',\n",
    "                 'Year',\n",
    "                 'YearSegment',\n",
    "                 'KMcluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Sampling started')\n",
    "tr = train_raw[model_fields+['Category']].copy().iloc[\\\n",
    "                   np.random.permutation(len(train_raw))]\n",
    "\n",
    "logger.info('Creating dev and test datasets')\n",
    "dev_train\\\n",
    "    , dev_train_labels_cat = tr[model_fields][50001:],\\\n",
    "                                tr['Category'][50001:]\n",
    "dev_test\\\n",
    "    , dev_test_labels_cat = tr[model_fields][:50000],\\\n",
    "                            tr['Category'][:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_pipeline(model_fields,categorical_fields):\n",
    "    ''' Create pipeline that will be \n",
    "        used multiple times.\n",
    "        \n",
    "        Run test model to see what performance is \n",
    "        with current set of features. \n",
    "    '''\n",
    "    le = MultiColumnLabelEncoder()\n",
    "    cf = [i for i,x in enumerate(model_fields) if x in categorical_fields]\n",
    "    ohe = OneHotEncoder(categorical_features=cf,sparse=True)\n",
    "    svd = TruncatedSVD(n_components=10) \n",
    "    ss = StandardScaler()\n",
    "    lr = LogisticRegression(C=0.01,solver='lbfgs'\\\n",
    "                            ,multi_class='multinomial')\n",
    "    pl = Pipeline([('le',le)        # Recode text features as integers with LabelEncoder\n",
    "                   ,('ohe',ohe)     # Create dummy features for each categorical feature\n",
    "                   ,('svd',svd)     # Decompose features into a smaller projection \\\n",
    "                                    #   as a dense matrix for scaler \n",
    "                   ,('ss',ss)       # Scale numerical variables \n",
    "                   ,('lr',lr)])     # Predict with LogisticRegression\n",
    "                   \n",
    "    return pl\n",
    "\n",
    "pl = make_pipeline(model_fields,categorical_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_category(pl):\n",
    "    '''Predict the category of crime\n",
    "    '''\n",
    "    \n",
    "    logger.info('Fitting training data') \n",
    "    pl.fit(dev_train, dev_train_labels_cat)\n",
    "    return pl\n",
    "\n",
    "logger.info('1st model for category started')\n",
    "pl_category = predict_category(pl)\n",
    "logger.info('1st model for category ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Model diagnostics started')\n",
    "logger.info('Model accuracy: {}%'.format(round(pl_category.score(dev_test, \\\n",
    "                                             dev_test_labels_cat),4)*100))\n",
    "predictions = pl_category.predict(dev_test)\n",
    "\n",
    "conf = confusion_matrix(dev_test_labels_cat, predictions)\n",
    "plt.imshow(conf, cmap='binary',interpolation='nearest')\n",
    "print pd.crosstab(dev_test_labels_cat, predictions, \\\n",
    "                  rownames=['True'], colnames=['Predicted'], \\\n",
    "                  margins=True)\n",
    "print 'F1 Score: {}%'.format(round(f1_score(\\\n",
    "                    dev_test_labels_cat\n",
    "                    ,predictions\\\n",
    "                    ,average='weighted')*100,2))\n",
    "print classification_report(\\\n",
    "                    dev_test_labels_cat\\\n",
    "                    ,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Get data from other sources\n",
    "\n",
    "SF OpenData has a ton of supplemental data sources that will be great to try out for this effort.  \n",
    "\n",
    "NOTE: one of them is actually a list of crimes that seems to match data in the training set. I will NOT use that data to train my model or match against the test dataset; however, I believe that many contestants are doing this, given that there is a very clear separation in scores that indicates to me that maybe there is some cheating happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://spatialreference.org/ref/epsg/2227/\n",
    "p = Proj('+proj=lcc +init=EPSG:2227 +datum=NAD83 +units=us-ft +no_defs',preserve_units=True)\n",
    "convert_vals = np.vectorize(lambda x,y: p(x,y))\n",
    "convert_vals_inv = np.vectorize(lambda x,y: p(x,y,inverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shpfilename_elect = 'Data/SanFranciscoElectricityUse/SanFranciscoElectricityUse.shp'\n",
    "shpfilename_school = 'Data/schools_public_pt/schools_public_pt.shp'\n",
    "shpfilename_neighborhoods = 'Data/planning_neighborhoods/planning_neighborhoods.shp'\n",
    "shpfilename_jobdensity = 'Data/SanFranciscoJobDensity/SanFranciscoJobDensity.shp'\n",
    "shpfilename_income = 'Data/SanFranciscoIncome/SanFranciscoIncome.shp'\n",
    "shpfilename_sfpdplots = 'Data/sfpd_plots/sfpd_plots.shp'\n",
    "shpfilename_sfpdsectors = 'Data/sfpd_sectors/sfpd_sectors.shp'\n",
    "shpfilename_employment = 'Data/SanFranciscoEmploymentRate/SanFranciscoEmploymentRate.shp'\n",
    "shpfilename_speeding = 'Data/SanFranciscoSpeedLimitCompliance/SanFranciscoSpeedLimitCompliance.shp'\n",
    "street_tree_locations = 'Data/Street_Tree_List.csv'\n",
    "business_locations = 'Data/Registered_Business_Map.csv'\n",
    "park_locations = 'Data/Park_and_Open_Space_Map.csv'\n",
    "offstreet_parking_locations = 'Data/Off-street_parking_lots_and_parking_garages_map.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert lat/lon to coordinates that match shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_raw['New_X'], train_raw['New_Y'] = \\\n",
    "    convert_vals(train_raw['X'],train_raw['Y'])\n",
    "test_raw['New_X'], test_raw['New_Y'] = \\\n",
    "    convert_vals(test_raw['X'],test_raw['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pair_fields = ['New_X','New_Y','X','Y']\n",
    "base_pairs = pd.concat([train_raw[pair_fields]\\\n",
    "                                 ,test_raw[pair_fields]])\n",
    "logger.info('Base pairs prior to duplicate removal: {}'.format(base_pairs.shape[0]))\n",
    "base_pairs = base_pairs.drop_duplicates()\n",
    "logger.info('Base pairs after duplicate removal: {}'.format(base_pairs.shape[0]))\n",
    "base_points = [ Point((x,y)) for x,y in zip(base_pairs['New_X'],base_pairs['New_Y']) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and process CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "trees = pd.read_csv(street_tree_locations)\n",
    "businesses = pd.read_csv(business_locations)\n",
    "parks = pd.read_csv(park_locations, quotechar = \"\\\"\")\n",
    "offstreet_parking = pd.read_csv(offstreet_parking_locations)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getll(x):\n",
    "    ''' Get lat/lon for 311 and parking file '''\n",
    "    try:\n",
    "        x = re.sub(r'[\\(\\)]','',x)\n",
    "        x = x.split(', ')\n",
    "        return float(x[0]),float(x[1])\n",
    "    except:\n",
    "        return (None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "offstreet_parking['LatLon'] = offstreet_parking['Location 1'].apply(getll)\n",
    "offstreet_parking['X'],offstreet_parking['Y'] = \\\n",
    "    offstreet_parking['LatLon'].apply(lambda x: x[1])\\\n",
    "    ,offstreet_parking['LatLon'].apply(lambda x: x[0])\n",
    "offstreet_parking['XCoord'], offstreet_parking['YCoord'] = \\\n",
    "    convert_vals(offstreet_parking['X'],offstreet_parking['Y'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getll(x):\n",
    "    ''' Get lat/lon for park & business file '''\n",
    "    try:\n",
    "        x = x.split('\\n')[2]\n",
    "        x = re.sub(r'[\\(\\)]','',x)\n",
    "        x = x.split(', ')\n",
    "        return float(x[0]),float(x[1])\n",
    "    except:\n",
    "        return (None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "parks['LatLon'] = parks['Location 1'].apply(getll)\n",
    "parks['X'] = parks['LatLon'].apply(lambda x: x[1])\n",
    "parks['Y'] = parks['LatLon'].apply(lambda x: x[0])\n",
    "parks['XCoord'], parks['YCoord'] = convert_vals(parks['X'],parks['Y'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "businesses['LatLon'] = businesses['Business_Location'].apply(getll)\n",
    "businesses['X'] = businesses['LatLon'].apply(lambda x: x[1])\n",
    "businesses['Y'] = businesses['LatLon'].apply(lambda x: x[0])\n",
    "businesses['XCoord'], businesses['YCoord'] = convert_vals(businesses['X'],businesses['Y'])\n",
    "businesses_class02 = businesses.loc[businesses['Class Code']=='02',['XCoord','YCoord']]\n",
    "businesses_class07 = businesses.loc[businesses['Class Code']=='07',['XCoord','YCoord']]\n",
    "businesses_class08 = businesses.loc[businesses['Class Code']=='08',['XCoord','YCoord']]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to find matches to crimes from supplemental data sources, based on location\n",
    "\n",
    "NOTE: Ideally these data sources would have some temporal element as well; however, most of them only contain data for a single time snapshot that is roughly in the middle of all the crimes data - better than nothing, but not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read shapefiles and store properties into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_speeding():\n",
    "    ''' Process speeding rates into pandas dataframe '''\n",
    "    shp = fiona.open(shpfilename_speeding)\n",
    "    n = len(shp)\n",
    "    Over_pct,\\\n",
    "    O5mph_pct,\\\n",
    "    Speed_avg = \\\n",
    "        np.empty(n,dtype=np.float),\\\n",
    "        np.empty(n,dtype=np.float),\\\n",
    "        np.empty(n,dtype=np.float)\n",
    "    for i,s in enumerate(shp):\n",
    "        Over_pct[i] = s['properties']['Over_pct']\n",
    "        O5mph_pct[i] = s['properties']['O5mph_pct']\n",
    "        Speed_avg[i] = s['properties']['Speed_avg']\n",
    "    shp.close()\n",
    "    props_df = pd.DataFrame({\n",
    "            'Id':[i+1 for i in range(n)],\\\n",
    "            'Over_pct':Over_pct,\\\n",
    "            'O5mph_pct':O5mph_pct,\\\n",
    "            'Speed_avg':Speed_avg\n",
    "        })\n",
    "    return props_df\n",
    "\n",
    "# props_df_speeding = read_speeding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_employment():\n",
    "    ''' Process employment rates into pandas dataframe '''\n",
    "    shp = fiona.open(shpfilename_employment)\n",
    "    n = len(shp)\n",
    "    Employ_pct,\\\n",
    "    Employ_moe = \\\n",
    "        np.empty(n,dtype='|S25'),\\\n",
    "        np.empty(n,dtype='|S25')\n",
    "    for i,s in enumerate(shp):\n",
    "        Employ_pct[i] = s['properties']['Employ_pct']\n",
    "        Employ_moe[i] = s['properties']['Employ_moe']\n",
    "    shp.close()\n",
    "    props_df = pd.DataFrame({\n",
    "            'Id':[i+1 for i in range(n)],\\\n",
    "            'Employ_pct':Employ_pct,\\\n",
    "            'Employ_moe':Employ_moe\n",
    "        })\n",
    "    return props_df\n",
    "\n",
    "props_df_employment = read_employment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_sfpdsectors():\n",
    "    ''' Process SFPD sectors into pandas dataframe '''\n",
    "    shp = fiona.open(shpfilename_sfpdsectors)\n",
    "    n = len(shp)\n",
    "    SECTORID = \\\n",
    "        np.empty(n,dtype='|S25')\n",
    "    for i,s in enumerate(shp):\n",
    "        SECTORID[i] = s['properties']['SECTORID']\n",
    "    shp.close()\n",
    "    props_df = pd.DataFrame({\n",
    "            'Id':[i+1 for i in range(n)],\n",
    "            'SECTORID':SECTORID\n",
    "        })\n",
    "    return props_df\n",
    "\n",
    "props_df_sfpdsectors = read_sfpdsectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_sfpdplots():\n",
    "    ''' Process SFPD plot into pandas dataframe '''\n",
    "    shp = fiona.open(shpfilename_sfpdplots)\n",
    "    n = len(shp)\n",
    "    PLOT = \\\n",
    "        np.empty(n,dtype='|S25')\n",
    "    for i,s in enumerate(shp):\n",
    "        PLOT[i] = s['properties']['PLOT']\n",
    "    shp.close()\n",
    "    props_df = pd.DataFrame({\n",
    "            'Id':[i+1 for i in range(n)],\n",
    "            'PLOT':PLOT\n",
    "        })\n",
    "    return props_df\n",
    "\n",
    "props_df_sfpdplots = read_sfpdplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_neighborhoods():\n",
    "    ''' Process neighborhood file into pandas dataframe '''\n",
    "    shp = fiona.open(shpfilename_neighborhoods)\n",
    "    n = len(shp)\n",
    "    NEIGHBORHO = \\\n",
    "        np.empty(n,dtype='|S25')\n",
    "    for i,s in enumerate(shp):\n",
    "        NEIGHBORHO[i] = s['properties']['neighborho']\n",
    "    shp.close()\n",
    "    props_df = pd.DataFrame({\n",
    "            'Id':[i+1 for i in range(n)],\n",
    "            'NEIGHBORHO':NEIGHBORHO\n",
    "        })\n",
    "    return props_df\n",
    "\n",
    "props_df_neighborhood = read_neighborhoods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_income():\n",
    "    ''' Process income file into pandas dataframe '''\n",
    "    shp = fiona.open(shpfilename_income)\n",
    "    n = len(shp)\n",
    "    MedInc_d,\\\n",
    "    MedInc_moe,\\\n",
    "    pC_Inc_d,\\\n",
    "    pC_Inc_moe = \\\n",
    "        np.empty(n,dtype='|S25'),\\\n",
    "        np.empty(n,dtype='|S25'),\\\n",
    "        np.empty(n,dtype='|S25'),\\\n",
    "        np.empty(n,dtype='|S25')\n",
    "    for i,s in enumerate(shp):\n",
    "        MedInc_d [i] = s['properties']['MedInc_d']\n",
    "        MedInc_moe [i] = s['properties']['MedInc_moe']\n",
    "        pC_Inc_d [i] = s['properties']['pC_Inc_d']\n",
    "        pC_Inc_moe [i] = s['properties']['pC_Inc_moe']\n",
    "    shp.close()\n",
    "    props_df = pd.DataFrame({\n",
    "            'Id':[i+1 for i in range(n)],\n",
    "            'MedInc_d':MedInc_d,\n",
    "            'MedInc_moe':MedInc_moe,\n",
    "            'pC_Inc_d':pC_Inc_d,\n",
    "            'pC_Inc_moe':pC_Inc_moe\n",
    "        })\n",
    "    return props_df\n",
    "\n",
    "props_df_income = read_income()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_jobdensity():\n",
    "    ''' Process job density file into pandas dataframe '''\n",
    "    shp = fiona.open(shpfilename_jobdensity)\n",
    "    n = len(shp)\n",
    "    JOBS_PSMI,\\\n",
    "    JOBS_CNT = \\\n",
    "        np.empty(n,dtype='|S25'),\\\n",
    "        np.empty(n,dtype='|S25')\n",
    "    for i,s in enumerate(shp):\n",
    "        JOBS_PSMI [i] = s['properties']['Jobs_psmi']\n",
    "        JOBS_CNT [i] = s['properties']['Jobs_cnt']\n",
    "    shp.close()\n",
    "    props_df = pd.DataFrame({\n",
    "            'Id':[i+1 for i in range(n)],\n",
    "            'JOBS_PSMI':JOBS_PSMI,\n",
    "            'JOBS_CNT':JOBS_CNT\n",
    "        })\n",
    "    return props_df\n",
    "\n",
    "props_df_jobs = read_jobdensity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_schoolfile():\n",
    "    ''' Process school file into pandas dataframe '''\n",
    "    shp = fiona.open(shpfilename_school)\n",
    "    n = len(shp)\n",
    "    SCHOOL_TYP,\\\n",
    "    DEPT,\\\n",
    "    FACILITY_N,\\\n",
    "    DEPTNAME,\\\n",
    "    FACILITY_I = \\\n",
    "        np.empty(n,dtype='|S25'),\\\n",
    "        np.empty(n,dtype='|S25'),\\\n",
    "        np.empty(n,dtype='|S25'),\\\n",
    "        np.empty(n,dtype='|S25'),\\\n",
    "        np.empty(n,dtype='|S25')\n",
    "    for i,s in enumerate(shp):\n",
    "        SCHOOL_TYP[i] = s['properties']['SCHOOL_TYP']\n",
    "        DEPT [i] = s['properties']['DEPT']\n",
    "        FACILITY_N [i] = s['properties']['FACILITY_N']\n",
    "        DEPTNAME [i] = s['properties']['DEPTNAME']\n",
    "        FACILITY_I [i] = s['properties']['FACILITY_I']\n",
    "    shp.close()\n",
    "    props_df = pd.DataFrame({\n",
    "            'Id':[i+1 for i in range(n)],\n",
    "            'SCHOOL_TYP':SCHOOL_TYP\n",
    "        })\n",
    "    return props_df\n",
    "\n",
    "props_df_schools = read_schoolfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_electfile():\n",
    "    shp = fiona.open(shpfilename_elect)\n",
    "    n = len(shp)\n",
    "    kWh_pC,\\\n",
    "    kWh,\\\n",
    "    Zip,\\\n",
    "    Pop2010_zc = \\\n",
    "        np.empty(n,dtype=np.float64),\\\n",
    "        np.empty(n,dtype=np.float64),\\\n",
    "        np.empty(n,dtype='|S10'),\\\n",
    "        np.empty(n,dtype=np.int64)\n",
    "    for i,s in enumerate(shp):\n",
    "        kWh_pC[i] = s['properties']['kWh_pC']\n",
    "        kWh [i] = s['properties']['kWh']\n",
    "        Zip [i] = s['properties']['Zip']\n",
    "        Pop2010_zc [i] = s['properties']['Pop2010_zc']\n",
    "    shp.close()\n",
    "\n",
    "    props_df_elect = pd.DataFrame({\n",
    "            'Id':[i+1 for i in range(n)],\\\n",
    "            'kWh_pC':kWh_pC,\\\n",
    "            'kWh':kWh,\\\n",
    "            'Zip':Zip,\\\n",
    "            'Pop2010_zc':Pop2010_zc\\\n",
    "        })\n",
    "    return props_df_elect\n",
    "\n",
    "props_df_elect = read_electfile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Polygon search functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polygon_search(shpfilename):\n",
    "    ''' Iterate through shapefile polygons\n",
    "        and find id of polygon for \n",
    "        each datapoint if it fits inside \n",
    "        of the polygon boundaries\n",
    "    '''\n",
    "    shp = pysal.open(shpfilename,'r')\n",
    "    pl = PolygonLocator([p for p in shp])\n",
    "    shp.close()\n",
    "    return pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Centroid search function (faster than polygon search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coord_search_centroid(shpfile,\\\n",
    "                 locator_fun=BruteForcePointLocator):\n",
    "    ''' Since polygon search is not very efficient\n",
    "        when there are many polygons, instead\n",
    "        do a comparison to each polygon centroid \n",
    "    '''\n",
    "    logger.info('Gathering centroids')\n",
    "    gather_centroids = lambda shp: [p.centroid for p in shp]\n",
    "\n",
    "    # Read file\n",
    "    shp = pysal.open(shpfile,'r')\n",
    "    centroids = gather_centroids(shp)\n",
    "    pl = locator_fun(centroids)\n",
    "    shp.close()\n",
    "    \n",
    "    return pl,centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions that iterate over each observation and match GIS data points\n",
    "These functions are used to divide the workload among multiple processors. Since the tasks are very CPU-bound and can be run separately then compiled afterward, this works quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_iters_points(points, point_locator, \\\n",
    "              proximity=100,log_at=100000):\n",
    "    ''' Iterate through points and return number\n",
    "        of points in surrounding proximity\n",
    "    '''\n",
    "    point_fun = lambda x,pl: pl.proximity(x,proximity)\n",
    "    surrounding_pts = np.zeros(len(points),dtype=np.int64)\n",
    "    for i,p in enumerate(points):\n",
    "        if i%log_at==0: logger.info('running {0} row'.format(i))\n",
    "        pts = point_fun(p,point_locator)\n",
    "        surrounding_pts[i] = len(pts)\n",
    "    return surrounding_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_iters_point_distance(points, point_locator\\\n",
    "            ,log_at=100000):\n",
    "    ''' Iterate through points and return distance\n",
    "        to the nearest point\n",
    "    '''\n",
    "    point_fun = lambda x,pl: pl.nearest(x)\n",
    "    distances = np.zeros(len(points),dtype=np.int64)\n",
    "    for i,p in enumerate(points):\n",
    "        if i%log_at==0: logger.info('running {0} row'.format(i))\n",
    "        pt = point_fun(p,point_locator)\n",
    "        distances[i] = arcdist(p,pt)\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_iters_poly(points, point_locator\\\n",
    "                   ,log_at=100000):\n",
    "    ''' Iterate through points and find matching polygon '''\n",
    "    \n",
    "    def return_poly_id(pl,x):\n",
    "        ''' Find the polygon within the \n",
    "            PolygonLocator that \n",
    "            matches to each point\n",
    "        '''\n",
    "        try:\n",
    "            return pl.contains_point(x)[0].id\n",
    "        except IndexError:\n",
    "            return -1\n",
    "    \n",
    "    poly_ids = np.zeros(len(points),dtype=np.int64)\n",
    "    for i,p in enumerate(points):\n",
    "        if i%log_at==0: logger.info('running {0} row'.format(i))\n",
    "        poly_ids[i] = return_poly_id(point_locator,p)\n",
    "    return poly_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_iters_centroid(points, point_locator, centroids, \\\n",
    "              log_at=1000):\n",
    "    ''' Iterate through points to find \n",
    "        the nearest matching polygon centroid point\n",
    "        \n",
    "        Faster than running polygon search \n",
    "    '''\n",
    "    logger.info('Running iterations on {} points'.format(len(points)))\n",
    "    point_fun = lambda x,pl: pl.nearest(x)\n",
    "    id_fun = lambda p, centroids: [i for i,c in enumerate(centroids)\\\n",
    "                                  if c==p][0]\n",
    "    nearest_ids = np.zeros(len(points),dtype=np.int64)\n",
    "    for i,p in enumerate(points):\n",
    "        if i%log_at==0: logger.info('running {0} row'.format(i))\n",
    "        pt = point_fun(p,point_locator)\n",
    "        pt_id = id_fun(pt, centroids)\n",
    "        nearest_ids[i] = pt_id\n",
    "    return nearest_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiprocessing function to distribute work over many cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_workload(worker,base_points,n_threads=2):\n",
    "    ''' Create subprocess threads and combine work \n",
    "        after finishing.\n",
    "        \n",
    "        NOTE: freezes in ipython notebook\n",
    "    '''\n",
    "    n = n_threads\n",
    "    # multiprocessing.freeze_support()\n",
    "    pool = multiprocessing.Pool(n)\n",
    "    \n",
    "    p = [ i*len(base_points)//n for i in range(n+1) ]\n",
    "\n",
    "    points_list = [ base_points[p[i]:p[i+1]] for i in range(n) ]\n",
    "    points_list += [base_points[p[n]:]]\n",
    "\n",
    "    res = pool.map(worker, points_list)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    x = pd.concat(res,axis=0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match crimes data to supplementary data sources\n",
    "\n",
    "The core of the work below was done on other machines, or on my laptop but outside of ipython notebook due to issues with parallel processing. In most cases, the code that was run is commented out, other than reading in the data from the files that were already processed. \n",
    "\n",
    "Most the matching is done based on lat/lon coordinates, which are not at all unique for each crime. In fact, only about 1.5% of the lat/lon coordinates are unique compared to the total number of crimes in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find nearby schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_schools(pts):\n",
    "    ''' Find nearest point and measure distance\n",
    "        for every datapoint \n",
    "    '''\n",
    "    hs = props_df_schools[props_df_schools['SCHOOL_TYP']=='High School']['Id']\n",
    "    cs = props_df_schools[props_df_schools['SCHOOL_TYP']=='County School']['Id']\n",
    "    chs = props_df_schools[props_df_schools['SCHOOL_TYP']=='Charter School']['Id']\n",
    "    ms = props_df_schools[props_df_schools['SCHOOL_TYP']=='Middle School']['Id']\n",
    "    em = props_df_schools[props_df_schools['SCHOOL_TYP']=='Elementary']['Id']\n",
    "    \n",
    "    shp = pysal.open(shpfilename_school,'r')\n",
    "    pl_hs = BruteForcePointLocator([p for p in shp if p.id in hs])\n",
    "    pl_cs = BruteForcePointLocator([p for p in shp if p.id in cs])\n",
    "    pl_chs = BruteForcePointLocator([p for p in shp if p.id in chs])\n",
    "    pl_ms = BruteForcePointLocator([p for p in shp if p.id in ms])\n",
    "    pl_em = BruteForcePointLocator([p for p in shp if p.id in em])\n",
    "    shp.close()\n",
    "\n",
    "    return_point_hs = lambda x: pl_hs.nearest(x)\n",
    "    return_point_cs = lambda x: pl_cs.nearest(x)\n",
    "    return_point_chs = lambda x: pl_chs.nearest(x)\n",
    "    return_point_ms = lambda x: pl_ms.nearest(x)\n",
    "    return_point_em = lambda x: pl_em.nearest(x)\n",
    "    \n",
    "    # point_ids = np.zeros(len(points),dtype=np.int8)\n",
    "    point_distances_hs = np.zeros(len(pts),dtype=np.float64)\n",
    "    point_distances_cs = np.zeros(len(pts),dtype=np.float64)\n",
    "    point_distances_chs = np.zeros(len(pts),dtype=np.float64)\n",
    "    point_distances_ms = np.zeros(len(pts),dtype=np.float64)\n",
    "    point_distances_em = np.zeros(len(pts),dtype=np.float64)\n",
    "    \n",
    "    def run_iters():\n",
    "        for i,p in enumerate(pts):\n",
    "            if i%1000==0: logger.info('running {0} row'.format(i))\n",
    "            pt_hs = return_point_hs(p)\n",
    "            pt_cs = return_point_cs(p)\n",
    "            pt_chs = return_point_chs(p)\n",
    "            pt_ms = return_point_ms(p)\n",
    "            pt_em = return_point_em(p)\n",
    "            \n",
    "            point_distances_hs[i] = arcdist(p,pt_hs)\n",
    "            point_distances_cs[i] = arcdist(p,pt_cs)\n",
    "            point_distances_chs[i] = arcdist(p,pt_chs)\n",
    "            point_distances_ms[i] = arcdist(p,pt_ms)\n",
    "            point_distances_em[i] = arcdist(p,pt_em)\n",
    "\n",
    "    run_iters()\n",
    "    \n",
    "    return point_distances_hs,\\\n",
    "            point_distances_cs,\\\n",
    "            point_distances_chs,\\\n",
    "            point_distances_ms,\\\n",
    "            point_distances_em\n",
    "\n",
    "'''\n",
    "point_distances_hs,\\\n",
    "            point_distances_cs,\\\n",
    "            point_distances_chs,\\\n",
    "            point_distances_ms,\\\n",
    "            point_distances_em = search_schools(base_points)\n",
    "schools = pd.DataFrame({'high school nearby':point_distances_hs\\\n",
    "                       ,'county school nearby':point_distances_cs\\\n",
    "                       ,'charter school nearby':point_distances_chs\\\n",
    "                       ,'middle school nearby':point_distances_ms\\\n",
    "                       ,'elementary school nearby':point_distances_em})\n",
    "schools['X'],schools['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "schools.to_csv('Data/schools.csv')\n",
    "'''\n",
    "schools = pd.read_csv('Data/schools.csv',header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find nearby parking lots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename='Data/parking.csv'\n",
    "'''\n",
    "offstreet_parking_points = [ Point((x,y)) for x,y in zip(offstreet_parking['XCoord'],offstreet_parking['YCoord']) ]\n",
    "def worker(points):\n",
    "    return pd.DataFrame({'nearest_parkinglot_distance':run_iters_point_distance(points\\\n",
    "                                  ,BruteForcePointLocator(offstreet_parking_points)\\\n",
    "                                  ,log_at=100000)})\n",
    "logger.info('Starting work on parking lots')\n",
    "x = do_workload(worker,base_points,8)\n",
    "x['X'],x['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x.to_csv(filename,index=True)\n",
    "'''\n",
    "parkinglots = pd.read_csv(filename,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process park CSV and find if park is nearby "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename='Data/parks.csv'\n",
    "'''\n",
    "park_points = [ Point((x,y)) for x,y in zip(parks['XCoord'],parks['YCoord']) ]\n",
    "def worker(points):\n",
    "    return pd.DataFrame({'nearest_park_distance':run_iters_point_distance(points\\\n",
    "                                  ,BruteForcePointLocator(park_points)\\\n",
    "                                  ,log_at=100000)})\n",
    "logger.info('Starting work on parks')\n",
    "x = do_workload(worker,base_points,8)\n",
    "x['X'],x['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x.to_csv(filename,index=True)\n",
    "'''\n",
    "parks = pd.read_csv(filename,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process major business classes nearby "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename02='Data/business_classes02.csv'\n",
    "filename07='Data/business_classes07.csv'\n",
    "filename08='Data/business_classes08.csv'\n",
    "'''\n",
    "bus02_points = [ Point((x,y)) for x,y in zip(businesses_class02['XCoord'],businesses_class02['YCoord']) ]\n",
    "bus07_points = [ Point((x,y)) for x,y in zip(businesses_class07['XCoord'],businesses_class07['YCoord']) ]\n",
    "bus08_points = [ Point((x,y)) for x,y in zip(businesses_class08['XCoord'],businesses_class08['YCoord']) ]\n",
    "def worker02(points):\n",
    "    return pd.DataFrame({'nearest_business_distance_02class':run_iters_point_distance(points\\\n",
    "                                  ,BruteForcePointLocator(bus02_points)\\\n",
    "                                  ,log_at=1000)})\n",
    "def worker07(points):\n",
    "    return pd.DataFrame({'nearest_business_distance_07class':run_iters_point_distance(points\\\n",
    "                                  ,BruteForcePointLocator(bus07_points)\\\n",
    "                                  ,log_at=1000)})\n",
    "def worker08(points):\n",
    "    return pd.DataFrame({'nearest_business_distance_08class':run_iters_point_distance(points\\\n",
    "                                  ,BruteForcePointLocator(bus08_points)\\\n",
    "                                  ,log_at=1000)})\n",
    "\n",
    "x02 = do_workload(worker02,base_points,8)\n",
    "x02['X'],x02['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x02.to_csv(filename02,index=True)\n",
    "\n",
    "x07 = do_workload(worker07,base_points,8)\n",
    "x07['X'],x07['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x07.to_csv(filename07,index=True)\n",
    "\n",
    "x08 = do_workload(worker08,base_points,8)\n",
    "x08['X'],x08['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x08.to_csv(filename08,index=True)\n",
    "'''\n",
    "business_class02 = pd.read_csv(filename02,index_col=0,header=0)\n",
    "business_class07 = pd.read_csv(filename07,index_col=0,header=0)\n",
    "business_class08 = pd.read_csv(filename08,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find neighborhood of crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename='Data/neighborhoods.csv'\n",
    "'''\n",
    "neighborhood_locator = polygon_search(shpfilename_neighborhoods)\n",
    "def worker(points):\n",
    "    return pd.DataFrame({'neighborhood_id':run_iters_poly(points\\\n",
    "                                    ,neighborhood_locator\n",
    "                                    ,log_at=10000)})\n",
    "\n",
    "logger.info('Starting work on neighborhood')\n",
    "x = do_workload(worker,base_points,8)\n",
    "x['X'],x['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x.to_csv(filename,index=True)\n",
    "'''\n",
    "neighborhoods = pd.read_csv(filename,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add SFPD Sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename='Data/sfpd_sectors.csv'\n",
    "'''\n",
    "sfpdsector_locator = polygon_search(shpfilename_sfpdsectors)\n",
    "def worker(points):\n",
    "    return pd.DataFrame({'sfpd_sector_id':run_iters_poly(points\\\n",
    "                                    ,sfpdsector_locator\n",
    "                                    ,log_at=10000)})\n",
    "\n",
    "logger.info('Starting work on SFPD Sectors')\n",
    "x = do_workload(worker,base_points,8)\n",
    "x['X'],x['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x.to_csv(filename,index=True)\n",
    "'''\n",
    "sfpd_sectors = pd.read_csv(filename,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add SFPD plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename='Data/sfpd_plots.csv'\n",
    "'''\n",
    "sfpdplot_locator = polygon_search(shpfilename_sfpdplots)\n",
    "def worker(points):\n",
    "    return pd.DataFrame({'sfpd_plot_id':run_iters_poly(points\\\n",
    "                                    ,sfpdplot_locator\n",
    "                                    ,log_at=10000)})\n",
    "\n",
    "logger.info('Starting work on SFPD Plots')\n",
    "x = do_workload(worker,base_points,8)\n",
    "x['X'],x['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x.to_csv(filename,index=True)\n",
    "'''\n",
    "sfpd_plots = pd.read_csv(filename,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process tree CSV and find number of trees nearby to crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename='Data/trees_wxy.csv'\n",
    "''' \n",
    "tree_points = [ Point((x,y)) for x,y in zip(trees['XCoord'],trees['YCoord']) ]\n",
    "\n",
    "def worker(points):\n",
    "    return pd.DataFrame({'trees':run_iters_points(points\\\n",
    "                                  ,BruteForcePointLocator(tree_points)\\\n",
    "                                  ,proximity=100\n",
    "                                  ,log_at=1000)})\n",
    "\n",
    "x = do_workload(worker,base_points,8)\n",
    "x.to_csv(filename,index=True)\n",
    "'''\n",
    "trees = pd.read_csv(filename,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process jobs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'Data/job_ids.csv'\n",
    "'''\n",
    "jobzone_locator = polygon_search(shpfilename_jobdensity)\n",
    "def worker(points):\n",
    "    return pd.DataFrame({'job_id':run_iters_poly(points\\\n",
    "                                    ,jobzone_locator\n",
    "                                    ,log_at=10000)})\n",
    "\n",
    "logger.info('Starting work on job zones')\n",
    "x = do_workload(worker,base_points,8)\n",
    "x['X'],x['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x.to_csv(filename,index=True)\n",
    "'''\n",
    "jobs = pd.read_csv(filename,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process employment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'Data/employment.csv'\n",
    "'''\n",
    "employment_locator = polygon_search(shpfilename_employment)\n",
    "def worker(points):\n",
    "    return pd.DataFrame({'employment_id':run_iters_poly(points\\\n",
    "                                    ,employment_locator\n",
    "                                    ,log_at=10000)})\n",
    "\n",
    "logger.info('Starting work on employment zones')\n",
    "x = do_workload(worker,base_points,8)\n",
    "x['X'],x['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x.to_csv(filename,index=True)\n",
    "'''\n",
    "employment = pd.read_csv(filename,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process income data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename='Data/income.csv'\n",
    "'''\n",
    "income_locator,centroids = coord_search_centroid(shpfilename_income)\n",
    "\n",
    "def worker(points):\n",
    "    return pd.DataFrame({'income_id':run_iters_centroid(points\\\n",
    "                                    ,income_locator\n",
    "                                    ,centroids\n",
    "                                    ,log_at=10000)})\n",
    "\n",
    "logger.info('Starting work on incomes')\n",
    "x = do_workload(worker,base_points,8)\n",
    "x['X'],x['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x.to_csv(filename,index=True)\n",
    "'''\n",
    "incomes = pd.read_csv(filename,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process electricity usage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename='Data/electricity.csv'\n",
    "'''\n",
    "elect_locator,centroids = coord_search_centroid(shpfilename_elect)\n",
    "\n",
    "def worker(points):\n",
    "    return pd.DataFrame({'electricity_id':run_iters_centroid(points\\\n",
    "                                    ,elect_locator\n",
    "                                    ,centroids\n",
    "                                    ,log_at=10000)})\n",
    "\n",
    "logger.info('Starting work on electricity')\n",
    "x = do_workload(worker,base_points,8)\n",
    "x['X'],x['Y'] = np.array(base_pairs['X']),np.array(base_pairs['Y'])\n",
    "x.to_csv(filename,index=True)\n",
    "'''\n",
    "electricity = pd.read_csv(filename,index_col=0,header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new columns\n",
    "Datasets matched by location:\n",
    "* incomes\n",
    "* employment\n",
    "* jobs\n",
    "* trees\n",
    "* neighborhoods\n",
    "* business_class02\n",
    "* business_class07\n",
    "* business_class08\n",
    "* parks\n",
    "* schools\n",
    "* sfpd_plots\n",
    "* sfpd_sectors\n",
    "* electricity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_fields = [#'IncidntNum',\n",
    "                 'Category',\n",
    "                 #'Descript',\n",
    "                 'DayOfWeek',\n",
    "                 #'Date',\n",
    "                 #'Time',\n",
    "                 'PdDistrict',\n",
    "                 #'Resolution',\n",
    "                 #'Address',\n",
    "                 'X',\n",
    "                 'Y',\n",
    "                 #'Location',\n",
    "                 #'PdId',\n",
    "                 'Dates',\n",
    "                 'collar_id',\n",
    "                 'DaySegment',\n",
    "                 'TimeOfDay',\n",
    "                 'DayOfMonth',\n",
    "                 'Year',\n",
    "                 'Month',\n",
    "                 'Year',\n",
    "                 'YearSegment',\n",
    "                 'KMcluster',\n",
    "                 'KMdistance',\n",
    "                 #'New_X',\n",
    "                 #'New_Y'\n",
    "              ]\n",
    "train_raw = train_raw[keep_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incomes = incomes.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "employment = employment.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "jobs = jobs.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "trees = trees.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "neighborhoods = neighborhoods.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "business_class02 = business_class02.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "business_class07 = business_class07.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "business_class08 = business_class08.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "parks = parks.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "schools = schools.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "sfpd_plots = sfpd_plots.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "sfpd_sectors = sfpd_sectors.drop_duplicates(subset=['X','Y'],take_last=True)\n",
    "electricity = electricity.drop_duplicates(subset=['X','Y'],take_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_mergekey(df):\n",
    "    ''' I had problems using pandas\n",
    "        merge on two float keys,\n",
    "        so I am creating a single \n",
    "        string mergekey and it works   \n",
    "    ''' \n",
    "    return (df['X'] * 10000000000).astype(str)\\\n",
    "            ,(df['Y'] * 10000000000).astype(str)\n",
    "\n",
    "def mergekeys():\n",
    "    logger.info('Creating mergekeys')\n",
    "    train_raw['X_merge'],train_raw['Y_merge'] = create_mergekey(train_raw)\n",
    "    train_raw['Datemerge'] = train_raw['Dates'].astype(str)\n",
    "    test_raw['X_merge'],test_raw['Y_merge'] = create_mergekey(test_raw)\n",
    "    test_raw['Datemerge'] = test_raw['Dates'].astype(str)\n",
    "    incomes['X_merge'],incomes['Y_merge'] = create_mergekey(incomes)\n",
    "    jobs['X_merge'],jobs['Y_merge'] = create_mergekey(jobs)\n",
    "    trees['X_merge'],trees['Y_merge'] = create_mergekey(trees)\n",
    "    neighborhoods['X_merge'],neighborhoods['Y_merge'] = create_mergekey(neighborhoods)\n",
    "    business_class02['X_merge'],business_class02['Y_merge'] = create_mergekey(business_class02)\n",
    "    business_class07['X_merge'],business_class07['Y_merge'] = create_mergekey(business_class07)\n",
    "    business_class08['X_merge'],business_class08['Y_merge'] = create_mergekey(business_class08)\n",
    "    parks['X_merge'],parks['Y_merge'] = create_mergekey(parks)\n",
    "    schools['X_merge'],schools['Y_merge'] = create_mergekey(schools)\n",
    "    electricity['X_merge'],electricity['Y_merge'] = create_mergekey(electricity)\n",
    "    employment['X_merge'],employment['Y_merge'] = create_mergekey(employment)\n",
    " \n",
    "mergekeys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_all():\n",
    "    for i,df in enumerate([train_raw,test_raw]):\n",
    "        t = 'train' if i==0 else 'test'\n",
    "        \n",
    "        logger.info('Merging income for {}'.format(t))\n",
    "        logger.info('Pre datasize: {}'.format(len(df)))\n",
    "        df = pd.merge(df,incomes[['income_id'\\\n",
    "                                   ,'X_merge','Y_merge']]\\\n",
    "                             ,on=['X_merge','Y_merge'],how='left',copy=False)\n",
    "\n",
    "        logger.info('Merging jobs for {}'.format(t))\n",
    "        df = pd.merge(df,jobs[['job_id'\\\n",
    "                                ,'X_merge','Y_merge']]\\\n",
    "                             ,on=['X_merge','Y_merge'],how='left')\n",
    "\n",
    "        logger.info('Merging trees for {}'.format(t))\n",
    "        df = pd.merge(df,trees[['trees'\\\n",
    "                                  ,'X_merge','Y_merge']]\\\n",
    "                             ,on=['X_merge','Y_merge'],how='left',copy=False)\n",
    "\n",
    "        logger.info('Merging neighborhoods for {}'.format(t))\n",
    "        df = pd.merge(df,neighborhoods[['neighborhood_id'\\\n",
    "                                        ,'X_merge','Y_merge']]\\\n",
    "                             ,on=['X_merge','Y_merge'],how='left',copy=False)\n",
    "\n",
    "        logger.info('Merging parks for {}'.format(t))\n",
    "        df = pd.merge(df,parks[['nearest_park_distance'\\\n",
    "                                ,'X_merge','Y_merge']]\\\n",
    "                             ,on=['X_merge','Y_merge'],how='left',copy=False)\n",
    "\n",
    "        logger.info('Merging business class 02 for {}'.format(t))\n",
    "        df = pd.merge(df,business_class02[\\\n",
    "                                        ['nearest_business_distance_02class'\\\n",
    "                                         ,'X_merge','Y_merge']]\\\n",
    "                             ,on=['X_merge','Y_merge'],how='left',copy=False)\n",
    "\n",
    "        logger.info('Merging business class 07 for {}'.format(t))\n",
    "        df = pd.merge(df,business_class07[['nearest_business_distance_07class'\\\n",
    "                                            ,'X_merge','Y_merge']]\\\n",
    "                             ,on=['X_merge','Y_merge'],how='left',copy=False)\n",
    "\n",
    "        logger.info('Merging business class 08 for {}'.format(t))\n",
    "        df = pd.merge(df,business_class08[['nearest_business_distance_08class'\\\n",
    "                                           ,'X_merge','Y_merge']]\\\n",
    "                             ,on=['X_merge','Y_merge'],how='left',copy=False)\n",
    "\n",
    "        logger.info('Merging electricity for {}'.format(t))\n",
    "        df = pd.merge(df,electricity[['electricity_id'\\\n",
    "                                      ,'X_merge','Y_merge']]\\\n",
    "                             ,on=['X_merge','Y_merge'],how='left',copy=False)\n",
    "\n",
    "        logger.info('Merging employment for {}'.format(t))\n",
    "        df = pd.merge(df,employment[['employment_id'\\\n",
    "                                      ,'X_merge','Y_merge']]\\\n",
    "                             ,on=['X_merge','Y_merge'],how='left',copy=False)\n",
    "\n",
    "        logger.info('Merging schools for {}'.format(t))\n",
    "        df = pd.merge(df,schools[['charter school nearby'\\\n",
    "                                    ,'county school nearby'\\\n",
    "                                    ,'elementary school nearby'\\\n",
    "                                    ,'high school nearby'\\\n",
    "                                    ,'middle school nearby'\\\n",
    "                                    ,'X_merge','Y_merge']]\\\n",
    "                             ,on=['X_merge','Y_merge'],how='left',copy=False)\n",
    "\n",
    "        logger.info('Merging electricity props for {}'.format(t))\n",
    "        df = pd.merge(df,props_df_elect,left_on='electricity_id'\\\n",
    "                                      ,right_on='Id'\\\n",
    "                                      ,how='left'\\\n",
    "                                      ,copy=False)\n",
    "\n",
    "        logger.info('Merging job props for {}'.format(t))\n",
    "        df = pd.merge(df,props_df_jobs,left_on='job_id'\\\n",
    "                                      ,right_on='Id'\\\n",
    "                                      ,how='left'\\\n",
    "                                      ,copy=False)\n",
    "\n",
    "        logger.info('Merging income props for {}'.format(t))\n",
    "        df = pd.merge(df,props_df_income,left_on='income_id'\\\n",
    "                                      ,right_on='Id'\\\n",
    "                                      ,how='left'\\\n",
    "                                      ,copy=False)\n",
    "\n",
    "        logger.info('Merging employment props for {}'.format(t))\n",
    "        df = pd.merge(df,props_df_employment,left_on='employment_id'\\\n",
    "                                      ,right_on='Id'\\\n",
    "                                      ,how='left'\\\n",
    "                                      ,copy=False)\n",
    "\n",
    "        logger.info('Post datasize: {}'.format(len(df)))\n",
    "        logger.info('Saving {} data to disk'.format(t))\n",
    "        filename = 'Data/merge_{}_all.csv'.format(t)\n",
    "        df.to_csv(filename)\n",
    "\n",
    "merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare datasets for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('Data/merge_{}_all.csv'.format('train'),index_col=0)\n",
    "test_raw = pd.read_csv('Data/merge_{}_all.csv'.format('test'),index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_fields = [#'Category',\n",
    "                 'DayOfWeek',\n",
    "                 'PdDistrict',\n",
    "                 #'collar_id',\n",
    "                 'DaySegment',\n",
    "                 'TimeOfDay',\n",
    "                 'DayOfMonth',\n",
    "                 'Year',\n",
    "                 'YearSegment',\n",
    "                 'KMcluster',\n",
    "                 'KMdistance',\n",
    "                 'trees',\n",
    "                 'neighborhood_id',\n",
    "                 'nearest_park_distance',\n",
    "                 'nearest_business_distance_02class',\n",
    "                 'nearest_business_distance_07class',\n",
    "                 'nearest_business_distance_08class',\n",
    "                 'charter school nearby',\n",
    "                 'county school nearby',\n",
    "                 'elementary school nearby',\n",
    "                 'high school nearby',\n",
    "                 'middle school nearby',\n",
    "                 'Pop2010_zc',\n",
    "                 'Zip',\n",
    "                 'kWh',\n",
    "                 'kWh_pC',\n",
    "                 'JOBS_CNT',\n",
    "                 'JOBS_PSMI',\n",
    "                 'MedInc_d',\n",
    "                 'MedInc_moe',\n",
    "                 'pC_Inc_d',\n",
    "                 'pC_Inc_moe',\n",
    "                 'Employ_moe',\n",
    "                 'Employ_pct']\n",
    "categorical_fields = [#'Category',\n",
    "                 'DayOfWeek',\n",
    "                 'PdDistrict',\n",
    "                 #'collar_id',\n",
    "                 'DaySegment',\n",
    "                 'TimeOfDay',\n",
    "                 'DayOfMonth',\n",
    "                 'Year',\n",
    "                 'YearSegment',\n",
    "                 'KMcluster',\n",
    "                 'neighborhood_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Sampling started')\n",
    "tr = train_raw[model_fields+['collar_id','Category']].copy().iloc[\\\n",
    "                   np.random.permutation(len(train_raw))]\n",
    "\n",
    "logger.info('Filling NA values')\n",
    "numerics = [col for col in model_fields \\\n",
    "            if col not in categorical_fields]\n",
    "tr[numerics] = tr[numerics].fillna(tr[numerics].mean())\n",
    "tr.loc[tr['neighborhood_id']==-1,['neighborhood_id']] = np.nan\n",
    "tr['neighborhood_id'] = tr['neighborhood_id'].fillna(99)\n",
    "\n",
    "logger.info('Creating dev and test datasets')\n",
    "dev_train\\\n",
    "    , dev_train_labels_cid\\\n",
    "    , dev_train_labels_cat = tr[model_fields][50001:],\\\n",
    "                             tr['collar_id'][50001:],\\\n",
    "                                tr['Category'][50001:]\n",
    "dev_test\\\n",
    "    , dev_test_labels_cid\\\n",
    "    , dev_test_labels_cat = tr[model_fields][:50000],\\\n",
    "                            tr['collar_id'][:50000],\\\n",
    "                            tr['Category'][:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Test data transform started')\n",
    "logger.info('Filling NA values')\n",
    "numerics = [col for col in model_fields \\\n",
    "            if col not in categorical_fields]\n",
    "test_raw[numerics] = test_raw[numerics].fillna(test_raw[numerics].mean())\n",
    "test_raw.loc[test_raw['neighborhood_id']==-1,['neighborhood_id']] = np.nan\n",
    "test_raw['neighborhood_id'] = test_raw['neighborhood_id'].fillna(99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a prediction of crime metaclass and use the prediction & prediction score in final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_pipeline(model_fields,categorical_fields):\n",
    "    ''' Create pipeline that will be \n",
    "        used multiple times.\n",
    "        \n",
    "        Pipeline contains all necessary\n",
    "        components of model run in \n",
    "        sequence. \n",
    "        \n",
    "        SVD runs prior to a RandomForest model\n",
    "        and creates a dense metrix. RF model\n",
    "        outputs new dataset with most important\n",
    "        features to LogisticRegression model.\n",
    "        \n",
    "        The same logic will be used twice:\n",
    "        1) predict collar ID and then add \n",
    "            the prediction and corresponding\n",
    "            prediction probability as features\n",
    "            on the original dataset\n",
    "        2) predict Category and use for \n",
    "            the next submission.\n",
    "    '''\n",
    "    le = MultiColumnLabelEncoder()\n",
    "    cf = [i for i,x in enumerate(model_fields) if x in categorical_fields]\n",
    "    ohe = OneHotEncoder(categorical_features=cf,sparse=True)\n",
    "    svd = TruncatedSVD(n_components=50) \n",
    "    ss = StandardScaler()\n",
    "    rf = RandomForestClassifier(n_estimators=100,n_jobs=1)\n",
    "    lr = LogisticRegression(C=0.01,solver='lbfgs'\\\n",
    "                            ,multi_class='multinomial')\n",
    "    pl = Pipeline([('le',le)        # Recode text features as integers with LabelEncoder\n",
    "                   ,('ohe',ohe)     # Create dummy features for each categorical feature\n",
    "                   ,('svd',svd)     # Decompose features into a smaller projection \\\n",
    "                                    #   as a dense matrix for scaler \n",
    "                   ,('ss',ss)       # Scale numerical variables \n",
    "                   ,('lr',lr)       # Transform data to most important features\n",
    "                   ,('rf',rf)])     # Run Forest model on minimized dataset\n",
    "                   \n",
    "    return pl\n",
    "\n",
    "pl = make_pipeline(model_fields,categorical_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_metaclass(pl):\n",
    "    '''Predict the metaclass of crime\n",
    "       Use the outputted algorithm to \n",
    "       predict metaclass in the training data\n",
    "    '''\n",
    "    \n",
    "    logger.info('Creating pipeline')\n",
    "    ''' GridSearch for optimal parameters\n",
    "    logger.info('GV search started')\n",
    "    param_grid = dict(md__C=[0.1,0.5,1,5])\n",
    "    gs = GridSearchCV(pl, param_grid=param_grid, verbose=10)\n",
    "    gs.fit(dev_train, dev_train_labels)\n",
    "    logger.info('Best C parameter: {}'.format(gs.best_estimator_)) \n",
    "    logger.info('GV search ended')\n",
    "    '''\n",
    "    \n",
    "    logger.info('Fitting training data') \n",
    "    pl.fit(dev_train, dev_train_labels_cid)\n",
    "    \n",
    "    return pl\n",
    "\n",
    "logger.info('Model for collar_id started')\n",
    "pl_metaclass = predict_metaclass(pl)\n",
    "logger.info('Model for collar_id ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Metaclass model diagnostics started')\n",
    "logger.info('Model accuracy: {}%'.format(round(pl_metaclass.score(dev_test, \\\n",
    "                                             dev_test_labels_cid),4)*100))\n",
    "predictions = pl_metaclass.predict(dev_test)\n",
    "conf = confusion_matrix(dev_test_labels_cid, predictions)\n",
    "plt.imshow(conf, cmap='binary', interpolation='nearest')\n",
    "print pd.crosstab(dev_test_labels_cid, predictions, \\\n",
    "                  rownames=['True'], colnames=['Predicted'], \\\n",
    "                  margins=True)\n",
    "print 'F1 Score: {}%'.format(round(f1_score(\\\n",
    "                    dev_test_labels_cid\\\n",
    "                    ,predictions\\\n",
    "                    ,average='weighted')*100,2))\n",
    "print classification_report(dev_test_labels_cid \\\n",
    "                    ,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add prediction and prediction score as a feature\n",
    "\n",
    "Use the generated dev and test datasets from above in future modeling efforts, because otherwise the model will have been fit with test data and accuracy measures will have been thrown off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_train['cid_prediction'] = pl_metaclass.predict(dev_train[model_fields])\n",
    "dev_train['cid_pred_score'] = \\\n",
    "    np.max(pl_metaclass.predict_proba(dev_train[model_fields]),axis=1)\n",
    "\n",
    "dev_test['cid_prediction'] = pl_metaclass.predict(dev_test[model_fields])\n",
    "dev_test['cid_pred_score'] = \\\n",
    "    np.max(pl_metaclass.predict_proba(dev_test[model_fields]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_test['cid_prediction'] = pl_metaclass.predict(dev_test[model_fields])\n",
    "dev_test['cid_pred_score'] = \\\n",
    "    np.max(pl_metaclass.predict_proba(dev_test[model_fields]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_raw['cid_prediction'] = pl_metaclass.predict(test_raw[model_fields])\n",
    "test_raw['cid_pred_score'] = \\\n",
    "        np.max(pl_metaclass.predict_proba(test_raw[model_fields]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make final model of crime category and test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_fields = model_fields + ['cid_prediction','cid_pred_score']\n",
    "categorical_fields = categorical_fields + ['cid_prediction']\n",
    "pl = make_pipeline(model_fields,categorical_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_category(pl):\n",
    "    '''Predict the category of crime\n",
    "        and use for submission.\n",
    "    '''\n",
    "    \n",
    "    logger.info('Fitting training data') \n",
    "    pl.fit(dev_train, dev_train_labels_cat)\n",
    "    return pl\n",
    "\n",
    "logger.info('Model for Category started')\n",
    "pl_category = predict_category(pl)\n",
    "logger.info('Model for Category ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info('Model diagnostics started')\n",
    "logger.info('Model accuracy: {}%'.format(round(pl_category.score(dev_test, \\\n",
    "                                             dev_test_labels_cat),4)*100))\n",
    "predictions = pl_category.predict(dev_test)\n",
    "conf = confusion_matrix(dev_test_labels_cat, predictions)\n",
    "plt.imshow(conf, cmap='binary',interpolation='nearest')\n",
    "print pd.crosstab(dev_test_labels_cat, predictions, \\\n",
    "                  rownames=['True'], colnames=['Predicted'], \\\n",
    "                  margins=True)\n",
    "print 'F1 Score: {}%'.format(round(f1_score(\\\n",
    "                    dev_test_labels_cat\n",
    "                    ,predictions\\\n",
    "                    ,average='weighted')*100,2))\n",
    "print classification_report(\\\n",
    "                    dev_test_labels_cat\\\n",
    "                    ,predictions\\)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = pl_category.predict(test_raw[model_fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_to_submissionfile():\n",
    "    l = len(test_raw)\n",
    "    submission = np.zeros((l,40),dtype=np.int32)\n",
    "    submission[:,0] = range(l)\n",
    "    cols = sorted(set(train_raw['Category']))\n",
    "    for i,c in enumerate(cols):\n",
    "        submission[:,i+1] = predictions == c\n",
    "\n",
    "    submission_cols = ['Id']\n",
    "    submission_cols.extend(cols)\n",
    "    submission_df = pd.DataFrame(submission,columns=submission_cols)\n",
    "    submission_df.rename(columns={'SEX OFFENSES, FORCIBLE':'SEX OFFENSES FORCIBLE'}, inplace=True)\n",
    "    submission_df.rename(columns={'SEX OFFENSES, NON FORCIBLE':'SEX OFFENSES NON FORCIBLE'}, inplace=True)\n",
    "    submission_df.head()\n",
    "\n",
    "    submission_df.to_csv('Data/submission_file_final.csv',index=False)\n",
    "    \n",
    "write_to_submissionfile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Cheating?\n",
    "\n",
    "Virtually all of the test dataset is available publicly online, with a holdout of only ~70k records. My thoughts are that the competition conductors will test using data from the holdout set (or else they are very silly). The problem is that without utilizing the full dataset you are at a severe disadvantage. I measure my own accuracy with a holdout set, but by more-than doubling my training set I can produce a stronger algorithm. I chose to follow the pattern of other contestants and utilize the full, publicly available dataset to create my final model. However, it really makes me wonder if people are just matching their predictions to the answer set... It also makes me wonder how my model generalizes in comparison to some of the other models out there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_raw_clean = test_raw.drop_duplicates()\n",
    "crimes = pd.read_csv('/Users/bshur/School/\\\n",
    "Machine Learning/Final Project/Data/\\\n",
    "SFPD_Incidents_-_from_1_January_2003.csv')\n",
    "crimes['Dates'] = pd.to_datetime(crimes['Date']\\\n",
    "                                 +' '\\\n",
    "                                 +crimes['Time']\\\n",
    "                                 ,format='%m/%d/%Y %H:%M')\n",
    "crimes_cleaned = crimes[['Dates','DayOfWeek'\\\n",
    "                         ,'PdDistrict','Address'\\\n",
    "                         ,'X','Y']].drop_duplicates()\n",
    "crimes_matched = pd.merge(crimes_cleaned\\\n",
    "                          ,test_raw_clean\\\n",
    "                          ,how='inner'\n",
    "                          ,on=['Dates','DayOfWeek'\\\n",
    "                               ,'PdDistrict','Address'\\\n",
    "                               ,'X','Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Test set count: {}\\nMatched set count: {}\\nDifference: {}'.format(\\\n",
    "                            test_raw_clean.shape[0]\\\n",
    "                            ,crimes_matched.shape[0]\\\n",
    "                            ,test_raw_clean.shape[0]\\\n",
    "                              -crimes_matched.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
